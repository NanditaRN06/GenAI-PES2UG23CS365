{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Nandita R Nadig<br>\n",
    "**SRN:** PES2UG23CS365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T03:59:56.075442Z",
     "iopub.status.busy": "2026-01-27T03:59:56.074691Z",
     "iopub.status.idle": "2026-01-27T04:00:04.960826Z",
     "shell.execute_reply": "2026-01-27T04:00:04.960218Z",
     "shell.execute_reply.started": "2026-01-27T03:59:56.075398Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-27 03:59:59.789000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769486399.811290     208 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769486399.818250     208 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769486399.835573     208 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769486399.835598     208 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769486399.835601     208 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769486399.835603     208 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T04:00:04.962771Z",
     "iopub.status.busy": "2026-01-27T04:00:04.962190Z",
     "iopub.status.idle": "2026-01-27T04:00:04.966340Z",
     "shell.execute_reply": "2026-01-27T04:00:04.965596Z",
     "shell.execute_reply.started": "2026-01-27T04:00:04.962742Z"
    }
   },
   "outputs": [],
   "source": [
    "model1 = \"bert-base-uncased\"       \n",
    "model2 = \"roberta-base\"             \n",
    "model3 = \"facebook/bart-base\"       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T04:00:04.967505Z",
     "iopub.status.busy": "2026-01-27T04:00:04.967235Z",
     "iopub.status.idle": "2026-01-27T04:00:04.983904Z",
     "shell.execute_reply": "2026-01-27T04:00:04.983371Z",
     "shell.execute_reply.started": "2026-01-27T04:00:04.967474Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T04:00:04.985037Z",
     "iopub.status.busy": "2026-01-27T04:00:04.984778Z",
     "iopub.status.idle": "2026-01-27T04:00:09.613607Z",
     "shell.execute_reply": "2026-01-27T04:00:09.612901Z",
     "shell.execute_reply.started": "2026-01-27T04:00:04.985007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT\")\n",
    "try:\n",
    "    gen1 = pipeline(\"text-generation\", model=model1)\n",
    "    print(gen1(prompt, max_length=30))\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Fails to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T04:00:09.615830Z",
     "iopub.status.busy": "2026-01-27T04:00:09.615529Z",
     "iopub.status.idle": "2026-01-27T04:00:10.566035Z",
     "shell.execute_reply": "2026-01-27T04:00:10.565327Z",
     "shell.execute_reply.started": "2026-01-27T04:00:09.615805Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"RoBERTa\")\n",
    "try:\n",
    "    gen2 = pipeline(\"text-generation\", model=model2)\n",
    "    print(gen2(prompt, max_length=30))\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Fails similarly to BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T04:00:10.567272Z",
     "iopub.status.busy": "2026-01-27T04:00:10.566968Z",
     "iopub.status.idle": "2026-01-27T04:00:13.706015Z",
     "shell.execute_reply": "2026-01-27T04:00:13.705148Z",
     "shell.execute_reply.started": "2026-01-27T04:00:10.567240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence isMary Israelis Israelisrations Israelis Israelis Mam Israelis drawingsrations IsraelisProt Israelis Philippine Israelis Israelis drawingsSeg Israelis Israelis Israelis Codex Israelis Israelis{{Prot custom Israelis connector drawings Israelis Israelisilibilib855 Wiley Wiley Wiley unders Wiley Wiley guySegSeg Wiley Wileyiking pouch Wiley guy Advisory walked Wiley intersectHistorySold Cobb Wiley Cobb IsraelisProt Wiley Wiley disciple SF Israelis Israelisittle Cobb industrySoldSold stylSoldSoldProtizons Cobb Israelis injuryRatProtSoldProtProt PleaseProtRat stone WileyizonsizonsSoldRatRatProtProtSold industryizonsSold styl Israelis IsraelisProtRat CobbProt Please discipleSoldRat stone squarely Israelisanny squarely 737Ratanny97RatRat IsraelisRatRat SunderlandRatRatannyRatProtRatRat hoursoceneizonsizonsProtProtizonsProtRatizonsizonsizons Caseyizonsizons Amnesty curiouslyRatRatRat pageantizonsо� wellizonsizons stoneizonsizonsannyannyizonsProtizonsSoldizons ministerialSoldRat translatorRatRat industry ministerial industry97 SecuritiesRat stoneampedProtizonsampedizonsizons97izons banizonsSoldITNESS Amnesty Amnesty well97Ratizonsannyizonsizons BreweryRatRatizons AmnestySold ministerializonsizonsRatizonsRatRat SecuritiesilationampedardsizonsSold industrySoldidepressizons Amnesty ministerialhidRatRatIsraeli ministerialRat well stone ministerializonsamped PleaseProtilationizonsizonshidilationizonsilation ministerialodgeizons ministerializonsRat'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"BART\")\n",
    "try:\n",
    "    gen3 = pipeline(\"text-generation\", model=model3)\n",
    "    print(gen3(prompt, max_length=30))\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Generates text but with weak quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T04:00:13.707219Z",
     "iopub.status.busy": "2026-01-27T04:00:13.706994Z",
     "iopub.status.idle": "2026-01-27T04:00:13.710915Z",
     "shell.execute_reply": "2026-01-27T04:00:13.710340Z",
     "shell.execute_reply.started": "2026-01-27T04:00:13.707197Z"
    }
   },
   "outputs": [],
   "source": [
    "masked_text1 = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "masked_text2 = \"The goal of Generative AI is to <mask> new content.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T04:00:13.711919Z",
     "iopub.status.busy": "2026-01-27T04:00:13.711670Z",
     "iopub.status.idle": "2026-01-27T04:00:14.312478Z",
     "shell.execute_reply": "2026-01-27T04:00:14.311760Z",
     "shell.execute_reply.started": "2026-01-27T04:00:13.711892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.5396896004676819, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575645864009857, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.054054491221904755, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}, {'score': 0.04451509192585945, 'token': 4503, 'token_str': 'develop', 'sequence': 'the goal of generative ai is to develop new content.'}, {'score': 0.017577331513166428, 'token': 5587, 'token_str': 'add', 'sequence': 'the goal of generative ai is to add new content.'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT\")\n",
    "fill1 = pipeline(\"fill-mask\", model=model1)\n",
    "print(fill1(masked_text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Accurately predicts the missing word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T04:00:14.313867Z",
     "iopub.status.busy": "2026-01-27T04:00:14.313448Z",
     "iopub.status.idle": "2026-01-27T04:00:15.002235Z",
     "shell.execute_reply": "2026-01-27T04:00:15.001511Z",
     "shell.execute_reply.started": "2026-01-27T04:00:14.313832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.37112897634506226, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.3677130341529846, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.08351433277130127, 'token': 8286, 'token_str': ' discover', 'sequence': 'The goal of Generative AI is to discover new content.'}, {'score': 0.021335072815418243, 'token': 465, 'token_str': ' find', 'sequence': 'The goal of Generative AI is to find new content.'}, {'score': 0.01652151718735695, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"RoBERTa\")\n",
    "fill2 = pipeline(\"fill-mask\", model=model2)\n",
    "print(fill2(masked_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Produces stronger and better-ranked predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T04:00:15.003279Z",
     "iopub.status.busy": "2026-01-27T04:00:15.003053Z",
     "iopub.status.idle": "2026-01-27T04:00:15.834997Z",
     "shell.execute_reply": "2026-01-27T04:00:15.834231Z",
     "shell.execute_reply.started": "2026-01-27T04:00:15.003257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.07461456954479218, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571789085865021, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}, {'score': 0.060879360884428024, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}, {'score': 0.03593526780605316, 'token': 3155, 'token_str': ' enable', 'sequence': 'The goal of Generative AI is to enable new content.'}, {'score': 0.033194273710250854, 'token': 1477, 'token_str': ' improve', 'sequence': 'The goal of Generative AI is to improve new content.'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"BART\")\n",
    "fill3 = pipeline(\"fill-mask\", model=model3)\n",
    "print(fill3(masked_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Predicts reasonable tokens but with lower confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T04:00:15.836150Z",
     "iopub.status.busy": "2026-01-27T04:00:15.835942Z",
     "iopub.status.idle": "2026-01-27T04:00:15.840643Z",
     "shell.execute_reply": "2026-01-27T04:00:15.840050Z",
     "shell.execute_reply.started": "2026-01-27T04:00:15.836130Z"
    }
   },
   "outputs": [],
   "source": [
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T04:00:15.841688Z",
     "iopub.status.busy": "2026-01-27T04:00:15.841491Z",
     "iopub.status.idle": "2026-01-27T04:00:17.853995Z",
     "shell.execute_reply": "2026-01-27T04:00:17.853386Z",
     "shell.execute_reply.started": "2026-01-27T04:00:15.841667Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.007884291931986809, 'start': 46, 'end': 71, 'answer': 'hallucinations, bias, and'}\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT\")\n",
    "qa1 = pipeline(\"question-answering\", model=model1)\n",
    "print(qa1(question=question, context=context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Produces incomplete or noisy answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T04:00:17.855091Z",
     "iopub.status.busy": "2026-01-27T04:00:17.854848Z",
     "iopub.status.idle": "2026-01-27T04:00:20.099665Z",
     "shell.execute_reply": "2026-01-27T04:00:20.098862Z",
     "shell.execute_reply.started": "2026-01-27T04:00:17.855068Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.008101048413664103, 'start': 20, 'end': 81, 'answer': 'significant risks such as hallucinations, bias, and deepfakes'}\n"
     ]
    }
   ],
   "source": [
    "print(\"RoBERTa\")\n",
    "qa2 = pipeline(\"question-answering\", model=model2)\n",
    "print(qa2(question=question, context=context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Occasionally extracts relevant phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T04:00:20.101897Z",
     "iopub.status.busy": "2026-01-27T04:00:20.101655Z",
     "iopub.status.idle": "2026-01-27T04:00:22.114800Z",
     "shell.execute_reply": "2026-01-27T04:00:22.113982Z",
     "shell.execute_reply.started": "2026-01-27T04:00:20.101875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.059548850171267986, 'start': 68, 'end': 81, 'answer': 'and deepfakes'}\n"
     ]
    }
   ],
   "source": [
    "print(\"BART\")\n",
    "qa3 = pipeline(\"question-answering\", model=model3)\n",
    "print(qa3(question=question, context=context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Generates fluent and unreliable answers"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
